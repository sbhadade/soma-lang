;;; ═══════════════════════════════════════════════════════════════
;;; SOMA STANDARD LIBRARY — soma.stdlib v1.0
;;; Core routines every SOMA program can import
;;; Import with: .IMPORT "stdlib/soma.stdlib"
;;; ═══════════════════════════════════════════════════════════════

.SOMA    1.0.0
.ARCH    ANY
.SOMSIZE 8x8
.AGENTS  16
.EXPORT  som_train_full
.EXPORT  som_cluster
.EXPORT  agent_pool_init
.EXPORT  agent_pool_get
.EXPORT  vec_cosine_sim
.EXPORT  vec_normalize
.EXPORT  msg_rpc
.EXPORT  broadcast_sync

.DATA

pool_size    : INT   = 0
pool_free    : BOOL[64]
pool_handles : INT[64]
rpc_result   : MSG
rpc_done     : BOOL = false

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; SOM ROUTINES
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
.CODE

;;; ─────────────────────────────────────────────────
;;; som_train_full — run a complete SOM training pass
;;; Args: R0 = input vector, R1 = epoch count
;;; Modifies: S0 (lr), S1 (sigma), S2 (epoch), S3 (bmu)
;;; ─────────────────────────────────────────────────
@som_train_full:
  STORE   [S2], R1             ;;; save epoch count

@stf_loop:
  LOAD    R2, [S2]
  JZ      R2, @stf_done

  ;;; find best matching unit
  SOM_BMU   R0, R3            ;;; input → BMU coord in R3
  STORE   [S3], R3             ;;; cache BMU

  ;;; compute neighborhood
  SOM_NBHD  R3, R4            ;;; BMU coord → sigma in R4

  ;;; update all weights in neighborhood
  WGHT_UPD  R0, R4            ;;; input, sigma

  ;;; decay learning rate
  LR_DECAY  0.002

  ;;; decrement epoch
  SUB     R2, R2, 0x01
  STORE   [S2], R2
  JMP     @stf_loop

@stf_done:
  RET

;;; ─────────────────────────────────────────────────
;;; som_cluster — find cluster ID for input vector
;;; Args: R0 = input vector
;;; Returns: R0 = cluster ID (BMU node index)
;;; ─────────────────────────────────────────────────
@som_cluster:
  SOM_BMU   R0, R1            ;;; → BMU coord
  SOM_SENSE R2                ;;; read activation
  MOV     R0, R1              ;;; return coord as ID
  RET

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; AGENT POOL ROUTINES
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

;;; ─────────────────────────────────────────────────
;;; agent_pool_init — initialize N reusable agents
;;; Args: R0 = pool size, R1 = @worker_entry
;;; ─────────────────────────────────────────────────
@agent_pool_init:
  STORE   [pool_size], R0
  ZERO    R2                   ;;; index

@api_loop:
  JEQ     R2, R0, @api_done
  SPAWN   A0, R1              ;;; spawn worker
  STORE   [pool_handles + R2], A0
  MOV     R3, 0x01
  STORE   [pool_free + R2], R3 ;;; mark free
  ADD     R2, R2, 0x01
  JMP     @api_loop

@api_done:
  RET

;;; ─────────────────────────────────────────────────
;;; agent_pool_get — grab a free agent from pool
;;; Returns: A0 = agent handle, R0 = index or -1
;;; ─────────────────────────────────────────────────
@agent_pool_get:
  ZERO    R0
  MOV     R1, [pool_size]

@apg_scan:
  JEQ     R0, R1, @apg_none
  LOAD    R2, [pool_free + R0]
  JNZ     R2, @apg_found
  ADD     R0, R0, 0x01
  JMP     @apg_scan

@apg_found:
  ZERO    R2
  STORE   [pool_free + R0], R2 ;;; mark busy
  LOAD    A0, [pool_handles + R0]
  RET

@apg_none:
  MOV     R0, 0xFFFFFFFF      ;;; -1 = no free agent
  RET

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; VECTOR MATH ROUTINES
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

;;; ─────────────────────────────────────────────────
;;; vec_cosine_sim — cosine similarity of two vectors
;;; Args: R0 = vec A, R1 = vec B
;;; Returns: R0 = similarity (f32 in low 32 bits)
;;; ─────────────────────────────────────────────────
@vec_cosine_sim:
  DOT     R2, R0, R1          ;;; dot product → R2
  NORM    R3, R0              ;;; |A| → R3
  NORM    R4, R1              ;;; |B| → R4
  MUL     R5, R3, R4          ;;; |A|*|B| → R5
  DIV     R0, R2, R5          ;;; dot/(|A||B|) → R0
  RET

;;; ─────────────────────────────────────────────────
;;; vec_normalize — normalize vector in-place
;;; Args: R0 = vector register
;;; Returns: R0 = normalized vector
;;; ─────────────────────────────────────────────────
@vec_normalize:
  NORM    R1, R0
  DIV     R0, R0, R1
  RET

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; MESSAGING ROUTINES
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

;;; ─────────────────────────────────────────────────
;;; msg_rpc — send message and block for reply
;;; Args: A0 = target agent, R0 = payload
;;; Returns: R0 = reply payload
;;; ─────────────────────────────────────────────────
@msg_rpc:
  MOV     R1, 0x00
  STORE   [rpc_done], R1
  MSG_SEND  A0, R0            ;;; send request
  WAIT    A0                  ;;; block for reply
  MSG_RECV  R0                ;;; read reply
  RET

;;; ─────────────────────────────────────────────────
;;; broadcast_sync — broadcast and wait for N acks
;;; Args: R0 = payload, R1 = expected ack count
;;; ─────────────────────────────────────────────────
@broadcast_sync:
  BROADCAST R0
  BARRIER   R1                ;;; wait for N agents
  RET

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; PHASE III: CURIOSITY / SOUL ROUTINES (v4.0)
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
.EXPORT  soul_init
.EXPORT  soul_learn_loop
.EXPORT  soul_on_stall
.EXPORT  terrain_explore
.EXPORT  terrain_deposit_and_die

;;; ─────────────────────────────────────────────────
;;; soul_init — set goal and read terrain before main loop
;;; Args:    R0 = goal vector (from MSG_RECV), A0 = parent
;;; Outputs: goal set in soul, R1 = terrain info
;;; ─────────────────────────────────────────────────
@soul_init:
  TERRAIN_READ  R1              ;;; read collective wisdom at current position
  GOAL_SET      R0              ;;; encode intended future state
  RET

;;; ─────────────────────────────────────────────────
;;; soul_learn_loop — one iteration of curiosity-aware learning
;;; Args:    R0 = current input vector, S0 = learning rate
;;; Outputs: R1 = goal distance; jumps to @soul_stall_handler if stalled
;;; Clobbers: R2
;;; ─────────────────────────────────────────────────
@soul_learn_loop:
  SOM_BMU       R0              ;;; find best matching unit
  SOM_TRAIN     R0, S0          ;;; train map toward input
  LR_DECAY      0x002           ;;; decay lr by 0.2% per iteration
  EMOT_TAG      S0, 0x3FFF      ;;; mild positive tag at current node
  TERRAIN_MARK  R0              ;;; update terrain with emotional state
  GOAL_CHECK    R1              ;;; distance to goal → R1
  GOAL_STALL    @soul_stall_handler  ;;; jump if stall_count > threshold
  RET

;;; ─────────────────────────────────────────────────
;;; soul_on_stall — standard curiosity response to stalled goal
;;; Args:    A0 = winner agent handle slot, R0 = goal_template
;;; Side effects: spawns 4 candidates, selects winner, transfers soul
;;; ─────────────────────────────────────────────────
@soul_on_stall:
  INTROSPECT                    ;;; know thyself before acting
  META_SPAWN    4, @_candidate  ;;; spawn 4 mutated-goal candidates
  BARRIER       4               ;;; wait for all candidates to complete
  EVOLVE        A0              ;;; select winner by goal proximity
  SOUL_INHERIT  A0              ;;; winner inherits this agent's memory
  RET

;;; ─────────────────────────────────────────────────
;;; soul_stall_handler — placeholder label (user must define)
;;; User programs should define @soul_stall_handler or call soul_on_stall
;;; ─────────────────────────────────────────────────
;;; (no code — label only, user overrides)

;;; ─────────────────────────────────────────────────
;;; terrain_explore — navigate toward most curious (virgin) node
;;; Args:    none
;;; Outputs: agent moved to most-curious SOM coordinate
;;; ─────────────────────────────────────────────────
@terrain_explore:
  TERRAIN_READ  R2              ;;; read current node terrain score
  JNZ           R2, @te_walk   ;;; if already visited, walk
  MOV           R3, 0x7FFF     ;;; discovery valence = +1.0
  TERRAIN_MARK  R3             ;;; mark as positively visited
  RET
@te_walk:
  SOM_WALK      SELF, GRADIENT  ;;; move toward high-activation neighbor
  RET

;;; ─────────────────────────────────────────────────
;;; terrain_deposit_and_die — dying agent leaves soul in terrain
;;; Args:    none (uses current agent state)
;;; Side effects: marks terrain, emits CDBG frame, kills self
;;; ─────────────────────────────────────────────────
@terrain_deposit_and_die:
  SOUL_QUERY    R3              ;;; query own soul vs current weight pattern
  TERRAIN_MARK  R3              ;;; deposit accumulated wisdom into terrain
  CDBG_EMIT                     ;;; broadcast identity frame to bus
  AGENT_KILL    SELF

;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
;;; PHASE IV: CDBG ROUTINES (v4.0)
;;; ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
.EXPORT  cdbg_announce
.EXPORT  cdbg_emit_emotion
.EXPORT  cdbg_listen_loop

;;; ─────────────────────────────────────────────────
;;; cdbg_announce — emit agent identity frame
;;; Args:    none
;;; Side effects: 5-byte AGENT frame sent to bus
;;; ─────────────────────────────────────────────────
@cdbg_announce:
  CDBG_EMIT                     ;;; CTX=AGENT, payload=24-bit agent ID, CRC-4
  RET

;;; ─────────────────────────────────────────────────
;;; cdbg_emit_emotion — emit emotional state as CDBG EMOTION frame
;;; Args:    R0 = valence (0–255), R1 = intensity (0–255)
;;; ─────────────────────────────────────────────────
@cdbg_emit_emotion:
  CTX_SWITCH    5               ;;; set CTX = EMOTION (0x5)
  CDBG_EMIT                     ;;; emit with current emotional registers
  CTX_SWITCH    1               ;;; restore CTX = AGENT
  RET

;;; ─────────────────────────────────────────────────
;;; cdbg_listen_loop — receive and dispatch CDBG frames
;;; Args:    none
;;; Exits when MSG_RECV returns 0 (no more frames)
;;; R0 = decoded frame, R1 = CTX nibble
;;; ─────────────────────────────────────────────────
@cdbg_listen_loop:
  CDBG_RECV     R0              ;;; decode incoming frame → R0
  JZ            R0, @cdbg_done ;;; 0 means no frame / error
  JMP           @cdbg_listen_loop
@cdbg_done:
  RET
